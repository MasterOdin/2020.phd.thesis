\section{Formalizing the Requirements for a CAIS}\label{sec:formalization_cais}

It should be noted that a CAIS to be considered a truly ``intelligent''
room, it is not sufficient that the room be intelligent about, for
example, search queries over a domain $D$; the room should also be
intelligent about cognitive states of agents in the room and their
cognitive states towards $D$.

Despite there being a significant amount of work done in building
intelligent environments (of varying levels of intelligence;
\cite{coen_design_1998, brooks_intelligent_1997,chan_review_2008}), there
is no formalization of what constitutes an intelligent room and what
separates it from an intelligent agent.  Though \cite{coen_design_1998}
briefly differentiates an intelligent room from ubiquitous computing
based on the non-ubiquity of sensors in the former, there is not any
formal or rigorous discussion of what separates an intelligent room
from a mobile robot that roams around the room with an array of
sensors.  We offer below a sketch of informal requirements that an
immersive room should aim for.  Then we instantiate these requirements
using \CEC.

Assuming that these two conditions hold, we can use them to make determinations
about the properties of such systems, as well as to help distinguish the various
classes of intelligent rooms and their capabilities that have been historically created.
%\subsection{Formal Requirements for a CAIS}
%\label{sect:freqs}
From the information requirements, we look to translate them into a formalized
version for a CAIS. The formal requirements are explained below:

    \begin{mdframed}[frametitle=Formal Requirements for $\mathcal{C}$ ,
      %frametitlebackgroundcolor=gray!25,
      nobreak,
      linecolor=white,
      %backgroundcolor=gray!10
      ]
      Assume $\Gamma \vdash t < t + \Delta$
      \begin{enumerate}
      \item[$\mathbf{C}^f_1:$] It is common knowledge that, if an
        agent $x$ has a false belief, $\gamma$ informs the agent of
        the belief:\footnote{Please note that inference in \CEC\ is
          non-monotonic as it includes the event calculus, which is
          non-monotonic.  If an agent $a$ believes $\phi$ based on
          prior information, adding new information can cause the
          agent to not believe $\phi$.}
         \begin{equation*}
          \common\color{gray!80}\left(\color{black}t, \left[\begin{aligned}
                &  \believes(\gamma, t, \phi) \land \believes(\gamma, t,
                \believes\big(x, t, \lnot \phi)\big)
                \\ & \hspace{50pt} \rightarrow \\
                & \hspace{30pt}\says(\gamma, x, t + \Delta, \phi)
              \end{aligned}\right]\color{gray!80}\right)\color{black}
        \end{equation*}

      \item[$\mathbf{C}^f_2:$] It is common knowledge that, if an
        agent $x$ has a missing belief, $\gamma$ informs the agent of
        the belief:
        \begin{equation*}
          \common\color{gray!80}\left(\color{black}t,\left[ \begin{aligned}
                &  \believes(\gamma, t, \phi) \land \believes(\gamma, t,
                \lnot \believes\big(x, t, \phi)\big)
                \\ &  \hspace{50pt}  \rightarrow \\
                & \hspace{30pt}\says(\gamma, x, t + \Delta, \phi)
              \end{aligned}\right]\color{gray!80}\right)\color{black}
        \end{equation*} \end{enumerate}
    \end{mdframed}

\begin{mdframed}[frametitle= Formal Requirements for $\mathcal{I}$ ,
  %frametitlebackgroundcolor=gray!25,
  nobreak,
  linecolor=white,
  %backgroundcolor=gray!10
  ]
  \begin{enumerate}
  \item[$\mathbf{I}^f_1:$] It is common knowledge that at any
    point in time $t$ an agent $x$, different from the CAIS system
    $\gamma$, can observe events or conditions (fluents) only in
    its vicinity.
    \begin{equation*}
      \begin{aligned}
        \common\Bigg(\forall x, t, f:& (x \not = \gamma) \rightarrow \Big[\perceives\big(x, t, \holds(f, t)\big) \rightarrow
        \holds(\vicinity(x, f), t)\Big]\Bigg)\\
        \common\Bigg(\forall x, t, e:& (x \not = \gamma) \rightarrow \Big[\perceives\big(x, t, \happens(e, t)\big) \rightarrow
        \holds(\vicinity(x, e), t)\Big]\Bigg)\\
      \end{aligned}
    \end{equation*}

  \item[$\mathbf{I}^f_2:$] It is common knowledge that actions
    performed by the agent are in its vicinity.
    \begin{equation*}
      \common\Big(\forall x, \alpha:  \holds\big(\vicinity(x, \action(a, \alpha)), t\big)\Big)
    \end{equation*}
  \item[$\mathbf{I}^f_3:$] It is common knowledge that all events
    and fluents are perceived by $\gamma$.  This is represented by
    the four conditions below:
    \begin{equation*}
      \begin{aligned}
   &  (i) \hspace{10pt}  \common\Bigg(\forall  t, f:  \Big[\holds(f, t) \leftrightarrow \perceives\big(\gamma, t, \holds(f, t)\big) \Big]\Bigg)\\
   & (ii)  \hspace{10pt}   \common\Bigg(\forall  t, e: \Big[\happens(e, t)
        \leftrightarrow \perceives\big(\gamma, t, \happens(e,
        t)\big) \Big]\Bigg)\\
    &   (iii)  \hspace{10pt}   \common\Bigg(\forall  t, f:  \Big[\lnot \holds(f, t)
        \leftrightarrow \perceives\big(\gamma, t, \lnot \holds(f, t)\big) \Big]\Bigg)\\
    &   (iv) \hspace{10pt}    \common\Bigg(\forall  t, e:\Big[\lnot \happens(e, t)
        \leftrightarrow \perceives\big(\gamma, t, \lnot \happens(e, t)\big) \Big]\Bigg)
      \end{aligned}
    \end{equation*}
  \end{enumerate}
\end{mdframed}

Time is assumed to be discrete, as in the
discrete event calculus presented in \cite{mueller_commonsense_2014}.  There is a
background set of axioms and propositions $\Gamma(t)$ that is
operational at time $t$.  We have a fluent $\vicinity$ that tells us
whether an agent is in the vicinity of a fluent, event, or another
agent:

$$\vicinity: \Agent \times \Fluent \cup \Event \cup \Agent \rightarrow
\Fluent $$

\noindent
Only events and fluents in the vicinity of an agent can be
observed by the agent.


One of the benefits of having a properly formalized CAIS is that we
can now state and prove various properties about these systems. If such
a system satisfies the above requirements, we can derive a foundationally
important (object-level in \CEC) property that we call:

\begin{small}
\begin{mdframed}[linecolor=white,frametitle=Expectation of Usefulness , %frametitlebackgroundcolor=gray!25,
%backgroundcolor=gray!10,
nobreak=true ,roundcorner=8pt]
  If the above properties hold, then an agent $a$ that perceives that
  another agent $b$ is not aware of an event happening, believes that
  CAIS $\gamma$ will inform $b$ (Assume: $\Gamma \vdash t < t + \Delta$):
\begin{equation*}
\begin{aligned}
&\perceives\big(a, t, \happens\big(e, t\big)\big) \land \perceives\Big(a, t, \lnot \holds\big(\vicinity(b, e),t\big)\Big) \\
& \hspace{90pt} \rightarrow \\
& \hspace{30pt} \believes\Big(a, t, \says\big(\gamma, b, t +\Delta,
\happens(e, t)\big)\Big)\\
\end{aligned}
\end{equation*}
\end{mdframed}
\end{small}

Through this property, humans within a CAIS can rely upon it to help other
agents with relevant missing or false information $\phi$, as opposed to the
case with a mobile robot. If we had a localized mobile robot instead of the
CAIS which failed to meet the above formal requirements, and therefore lacked
the above property, the human would need to decide whether or not the robot has
the required information $\phi$ and needs to believe that the robot believes that
another agent is missing the relevant information $\phi$. To help illustrate this
point, we provide a proof sketch using the inference schema of the \CEC\ and
the above formal requirements to derive the property:

% RPI says no color :(
%\footnote{We add color here
%to ease reading of the proof, and for tracing terms through.}

%\noindent{\textbf{Proof Sketch}}:


\begin{footnotesize}
    \begin{prooftree}
      \AxiomC{$\perceives(a, t, \happens(e, t))$}
      \LeftLabel{$D_{[\perceives \leadsto \knows]}$}   \UnaryInfC{$\knows(a, t, \happens(e, t))$}
     \LeftLabel{$D_{[\knows \leadsto \believes]}$}     \UnaryInfC{$\believes(a, t, \happens(e, t)) \equiv \boxed{\phi_1}$}
  \end{prooftree}

    \begin{prooftree}
      \AxiomC{$\perceives(a, t, \lnot \holds\big(\vicinity(b, e),t\big))$}
   \LeftLabel{$D_{[\perceives \leadsto \knows]}$}   \UnaryInfC{$\knows\Big(a, t, \lnot \holds\big(\vicinity(b, e),t\big)\Big)$}
     \LeftLabel{$D_{[\knows \leadsto \believes]}$}    \UnaryInfC{$\believes\Big(a, t, \lnot \holds\big(\vicinity(b, e),t\big)\Big) \equiv \boxed{\phi_2}$}
  \end{prooftree}
\end{footnotesize}

Using $\mathbf{I}^f_3(ii)$ the CAIS observes all events that happen in
its enclosure:
\begin{footnotesize}
\begin{prooftree}
        \AxiomC{$\mathbf{I}^f_3 (ii) \equiv\common\Bigg(\forall t, e: \Big[\happens(e, t) \leftrightarrow \perceives\big(\gamma, t, \happens(e, t)\big) \Big]\Bigg)$}
    \LeftLabel{$D_{[\common \leadsto \believes]}$  }
    \UnaryInfC{$\believes\left(a, t, \forall t, e: \Big[\happens(e, t)
            \leftrightarrow \perceives\big(\gamma, t, \happens(f, t)\big)
            \Big]\right)$}    \AxiomC{$\boxed{\phi_1}$}
    \LeftLabel{$R_{\mathbf{B}}$}   \BinaryInfC{$\believes\Big(a, t, \perceives\big(\gamma, t,
          \happens(e, t)\big)\Big)$}
    \LeftLabel{$R_{\mathbf{B}}$}  \UnaryInfC{$\believes \color{black}\big(\color{black} a, t, \color{black}\believes\big(\gamma, t, \happens(e,t)\big) \color{black}\big)\color{black} \equiv
    %\LeftLabel{$R_{\mathbf{B}}$}  \UnaryInfC{$\believes \color{black}\big(\color{black} a, t, \color{violet}\believes\big(\gamma, t, \happens(e,t)\big) \color{black}\big)\color{black} \equiv
   \boxed{\psi_1}$}
\end{prooftree}
\end{footnotesize}
Similarly, using $\mathbf{I}^f_3 (iii)$:
\begin{footnotesize}
\begin{prooftree}
        \AxiomC{$\mathbf{I}^f_3 (iii) \equiv \common\Bigg(\forall t,e : \Big[\lnot
          \holds(e, t)\leftrightarrow
          \perceives\big(\gamma, t,  \lnot
          \holds(e, t)\big) \Big]\Bigg)$}
    \LeftLabel{$D_{[\common \leadsto \believes]}$  }
\UnaryInfC{$\believes\left(a, t,\forall t, e\Big[\lnot
          \holds(e,t)\leftrightarrow
          \perceives\big(\gamma, t,  \lnot
          \holds(e, t)\big)\Big]\right)$}
\AxiomC{$\boxed{\phi_2}$}
     \LeftLabel{$R_{\mathbf{B}}$}   \BinaryInfC{$\believes(a, t,
       \perceives(\gamma, t, \lnot
          \holds\big(\vicinity(b, e), t)))$}
    \LeftLabel{$R_{\mathbf{B}}$}     \UnaryInfC{$\believes(a, t,
      \believes\big(\gamma, t,  \lnot \holds\big(\vicinity(b, e), t\big))
      \equiv \boxed{\phi_3}$}
\end{prooftree}
\end{footnotesize}
From $\mathbf{I}^f_1$ (and from $\believes\big(a, t, \believes(\gamma, t, b \not = \gamma))$):
\begin{footnotesize}
\begin{prooftree}
        \AxiomC{$\common\Big(\forall x, t,e: (x\not = \gamma)\rightarrow\big[
       \perceives\big(x, t,  \happens(e, t)\big) \rightarrow    \holds\big(\vicinity(x, e), t\big)
         \big]\Big)$}
    \LeftLabel{    $D_{[\common \leadsto \believes]}$  }
 \UnaryInfC{$\believes\big(a, t, \believes\big(\gamma, t, \forall x, t,e:  (x\not = \gamma)\rightarrow \big[
       \perceives\big(x, t,  \happens(e, t)\big) \rightarrow    \holds\big(\vicinity(x, e), t\big)
         \big]\big)\big)$}
\LeftLabel{    $R_{\believes}$  }
\UnaryInfC{$\believes\left(a, t, \believes\left(\gamma, t\Big[\lnot
          \holds\big(\vicinity(b, e), t\big)\rightarrow \lnot
          \perceives\big(b, t,  \happens(e, t)\big)\Big]\right)\right)$}
\AxiomC{$\boxed{\phi_3}$}
     \LeftLabel{$R_{\mathbf{B}}$}   \BinaryInfC{$\believes\Big(a, t,
       \believes\Big(\gamma, t, \lnot \perceives(b, t,
           \happens(e, t)\big)\Big)\Big)$}
    \LeftLabel{$R_{\mathbf{B}}$}
 \UnaryInfC{$\believes \color{black}\big(\color{black} a, t, \color{black} \believes(\gamma, t, \lnot \believes(b,
 %\UnaryInfC{$\believes \color{black}\big(\color{black} a, t, \color{NavyBlue} \believes(\gamma, t, \lnot \believes(b,
  t, \happens(e,t))) \color{black}\big)\color{black}  \equiv
   \boxed{\psi_2}$}
\end{prooftree}
\end{footnotesize}
From $\mathbf{C}^f_2$, we have:
\begin{scriptsize}
  \begin{prooftree}
    \AxiomC{
      $ \common\left(t,\left[
          \color{black} \believes(\gamma, t, \happens(e,t)) \color{black} \land \color{black}\believes(\gamma, t,
          %\color{violet} \believes(\gamma, t, \happens(e,t)) \color{black} \land \color{NavyBlue}\believes(\gamma, t,
                \lnot \believes\big(b, t, \happens(e,t))) \color{black} \rightarrow
                 \color{black} \says(\gamma, b, t + \Delta, \happens(e,t))\color{black}
                 %\color{OliveGreen} \says(\gamma, b, t + \Delta, \happens(e,t))\color{black}
              \right]\right)$}
\LeftLabel{$D_{[\common \leadsto \believes]}$}\UnaryInfC{
      $ \believes \color{black}\big(\color{black} a, t,\left[
        \color{black} \believes(\gamma, t, \happens(e,t)) \color{black} \land \color{black}\believes(\gamma, t,
        %\color{violet} \believes(\gamma, t, \happens(e,t)) \color{black} \land \color{NavyBlue}\believes(\gamma, t,
              \lnot \believes\big(b, t, \happens(e,t))) \color{black} \rightarrow
              \color{black} \says(\gamma, b, t + \Delta, \happens(e,t))\color{black}
              %\color{OliveGreen} \says(\gamma, b, t + \Delta, \happens(e,t))\color{black}
            \right]\color{black}\big) \color{black} \equiv \boxed{\psi}$}
\end{prooftree}
\end{scriptsize}
Using the above derived, $\boxed{\psi}$ and using $R_{\mathbf{B}}$:

\begin{footnotesize}
  \begin{prooftree}

\AxiomC{$\boxed{\psi}$}
\AxiomC{ $\believes \color{black}\big(\color{black} a, t, \color{black}\believes\big(\gamma, t, \happens(e,t)\big) \color{black}\big)\color{black} \equiv
%\AxiomC{ $\believes \color{black}\big(\color{black} a, t, \color{violet}\believes\big(\gamma, t, \happens(e,t)\big) \color{black}\big)\color{black} \equiv
   \boxed{\psi_1}$}
\AxiomC{$\believes \color{black}\big(\color{black} a, t, \color{black} \believes(\gamma, t, \lnot \believes(b,
%\AxiomC{$\believes \color{black}\big(\color{black} a, t, \color{NavyBlue} \believes(\gamma, t, \lnot \believes(b,
  t, \happens(e,t))) \color{black}\big)\color{black}  \equiv
   \boxed{\psi_2}$}
\LeftLabel{$R_{\mathbf{B}}$} \TrinaryInfC{$\believes(a, t,
  \color{black} \says(\gamma, b, t + \Delta,
  %\color{OliveGreen} \says(\gamma, b, t + \Delta,
  \happens(e,t))\color{black})$}

\end{prooftree} $\blacksquare$

\end{footnotesize}

\noindent The proof sketch given above demonstrates that common
knowledge that the CAIS will rectify false or missing information is
essential when multiple agents are present. If the CAIS rectifies
false or missing information but if this fact is not commonly known,
an agent might not expect the system to actively help another agent,
rendering the CAIS less useful.
