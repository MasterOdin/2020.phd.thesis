\section{Introduction}

Now that we have established both a framework for building out a CAIS and
a practical implementation, in this chapter we now establish the formal
schematics for our framework that we informally defined above. To accomplish
this, we require a highly expressive language capable of modeling agents'
cognitive states, including beliefs, knowledge, etc. These cognitive states
can range over physical details of the world at large, or even about the
cognitive states of other agents.

To accomplish this task, we introduce the \textit{Cognitive Event
Calculus} (\CEC), which is a highly expressive intentional logic that fits
our needs. We briefly provide a definition of the logic, and an description
of its syntax and sorts. Additionally, we briefly cover how we can handle
the logic in an automated fashion using ShadowProver. From this, we then
provide a formalization of the principles of a CAIS, and for how fundamenta
actions (such as pointing) are captured from the CAIS into the \CEC. We
close out the chapter tying together these efforts in demonstration of
solving a psychological test by the CAIS to demonstrate its capacity
for dealing with theory-of-mind reasoning.

It should be noted that we don't require from our AI overseeing an
environment populated with humans have so-called
\textit{phenomenal consciousness}, a form of ``what's it's like to''
consciousness characterized by \cite{bbs.block}, and
claimed by \cite{sb_billion_conscious_robot}, to be impossible for a
mere machine to possess.  In sharp contrast with phenomenal
consciousness, \textit{cognitive consciousness} consists only in the
logico-mathematical \emph{structure} of human-level (and, indeed,
above) cognition, instantiated through time.  While cognitive
consciousness can be characterized axiomatically with help from the
formal languages we introduce below for cognitive calculi
\cite{axiomatizing_consciousness1}, in the present paper we do not
require the AI overseeing a CAIS have even cognitive consciousness,
and we specifically do not require, at this early point in our work
on cognitive-and-immersive systems, cognitive
\emph{self}-consciousness, despite the fact that the latter is
something that has been significantly mechanized and implemented
\cite{roman2015_robot_self-con,sb_on_knowledge_game}.