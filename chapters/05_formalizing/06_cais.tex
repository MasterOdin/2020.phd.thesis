\section{Formalizing the Modules of our CAIS}

In this section, we now provide formalizations for the modules that exist within our
CAIS that exist across all use-cases. Going back to the discussion in 
Chapter~\ref{chap:technology}, we assume that a given CAIS implementation will minimally
feature some ability to point at content and an ability to input intents into the system,
such as through voice, typing natural language sentences, or clicking on content. We now
quickly go through these mechanisms here for usage within the \CEC. Additionally, for
several of the components, we provide a spectrum of how these formalizations come to bear
taking into account the potential feature-set of a given CAIS implementation.

\subsection{Users in CAIS}

One thing that is almost universally required in some capacity is understanding who is
utilizing the system. This is especially important within multi-agent scenarios, so
as to allow subscribing to each user the appropriate cognitive states from the rest
of the system. For each of the different environments a CAIS may run in, for each,
we assume that a mechanism of user registration exists. This mechanism, be it for
example a camera that utilizes facial recognition as users enter a room or users
simply announcing their presence triggers a registration action. The result of the
registration action is that the user is the user is now considered within the CAIS.
On the opposite side of things, as a lower leaves the CAIS, they will de-register
with the system, and now be considered outside of the CAIS. For a CAIS that has a
full compliment of sensors, some of these sensors would also provide positional
data about where exactly agents are within the system. For simplification of our
logic system so as to not incorporate axiomatization of mathematics a distance
fluent which is defined as the straight line distance between two agents as calculated
through the standard distance formula. From this, we add two
additional actions and three fluents:

\begin{equation*}
\begin{aligned}
  & \register: \Agent \times \Object \rightarrow \ActionType \\
  & \deregister: \Agent \rightarrow \ActionType \\
  & \inCAIS: \Agent \rightarrow \Fluent \\
  & \position: \Agent \times \Numeric \times \Numeric \rightarrow \Fluent \\
  & \distance: \Agent \times \Agent \times \Numeric \rightarrow \Fluent \\
\end{aligned}
\end{equation*}

\subsection{Perception and Vicinity}

From above, our $\textnormal{I}_{1}^{f}$, it is common knowledge that agents perceive
things that happen within their vicinity. However, we purposely left it vague as to
what constitutes being in an agent's vicinity. This was done purposefully so as to
handle the diverse range of environments, where information about an agent within
the space may be sparse or fine-grained. Take for example an environment with minimal
sensors, and only such that the CAIS is only able to track users who have registered
themselves. Here, vicinity is in its weakest form, where any agent who is within the CAIS
is then in the vicinity of any event or fluent within the CAIS. If the CAIS can track
positional data, then we may say that the vicinity of an agent is defined as within an
explicit distance of that agent. In its most strict form, for an environment that is
able to capture directional head pose information about users (or even more crucially
eye gaze), we can fully capture a user's perception, and can in some ways not utilize
vicinity. For our foundational work here, while we mention this case, we do not utilize it
due to a lack of technology that adequately supports this outside of highly staged
laboratory environments or the information is so coarse as to be largely equivalent to just
that the user is in the CAIS for our needs.

\subsection{Pointing}

We first focus on formalizing pointing in our system. While agents may point at anything 
within the room, we focus principally on agents pointing at content on a screen. To 
accomplish this,as was discussed earlier we utilize the Reagent system to allow us to
capture content  shown on arbitrary webpages. For each piece of meaningful content shown on 
the screen, we  assume that it can be expressed by some fluent. An agent then points at the 
fluent, and that information is captured by our CAIS. From this, we provide our
formalization of the point action:

\begin{equation*}
\begin{aligned}
  \point: \Agent \times \Object \rightarrow \ActionType
\end{aligned}
\end{equation*}

For this to operate without flooding our system within point actions, we only consider
``meaningful pointing'' events. These events are ones in which a user points at a piece
of content for a minimum of a second. This is to avoid registering ``interim pointing''
events, which occur as an agent moves their pointing across the screen to its final
destination.

\subsection{Spoken / Typed Natural Language Sentences and MUIFOLD Input}

Opposite to pointing is the other input mechanisms provided for interacting with our system.
Here, we focus on input from users that can be spoken or typed as natural language and
interactions from MUIFOLD to content on the screen. In all of these cases, the underlying
system translates the input into a formalized action relevant to the domain under study
and attributes it to the user committing the action. For the natural language sentences,
our conversation-worker handles converting the sentence into a $<I, E>$ tuple which
can then be directly matched onto a defined $\ActionType$ in our system. MUIFOLD in turn is
similarly augmented within our orchestrator to handle inputs from it into appropriate action
types.