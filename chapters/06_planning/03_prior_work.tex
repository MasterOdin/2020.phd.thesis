\section{Prior Work}

Planning, and more specifically plan recognition, is a rich field of
research that has a broad range of prior work that we draw from. For planning,
one branch of work is ``deductive planning'', wherein given a goal and a series 
of axioms,
the resulting proof gives a plan structure~\cite{green_application_1969}. This
approach is attractive as it allows the usage of first-order logic to define
our world, and the state transitions. Further work was done to handle concepts
of conditional branching, plan composition, and 
recursion~\cite{metzing_plan_1989,biundo_deductive_1992,rosenschein_plan_1981}.
However, through the regular use of FOL, we see a larger potential of ending
up with side-effects of actions, which can be undesirable for planning. An
alternative concept is through the usage of STRIPS-style 
planners~\cite{fikes_strips_1971}. Within these planners, each action is
defined via a complete structure with pre-conditions and post-conditions for
each action. This approach is attractive as it provides a useful mechanism for
dealing with the framing problem~\cite{mccarthy_philosophical_1969}. However,
these planners generally utilize a predicate calculus that lacks the full
expressiveness of first-order logic, not to mention higher order logics like
the \CEC. While STRIPS-styles planners work well across many domains, they
rely on deterministic planning. Extending the concept, conditional nonlinear
planners~\cite{peot_conditional_1992} and partial 
planners~\cite{pryor_planning_1996}
provide a mechanism for dealing with non-deterministic plans through if-else
like structures. While we do make usage of elements of these planners for
intent resolution, we avoid bringing in a full-on non-deterministic planner
so as to avoid the increased complexity and potential combinatorial explosion
of states as to do would also call for a limitation on the
expressivity~\cite{rintanen_constructing_1999}. As such, we also do
not concern ourselves with other extensions to classical planners such as 
probabilistic 
planners~\cite{boutilier_decision-theoretic_1999,kaelbling_planning_1998}.

We now examine applications of planners to related domain as us,
namely in operating over mental states of agents in group environments.
For example, work in human-robot teaming has focused on
the use of planning techniques that take human goals and
mental states into account~\cite{briggs_multi-modal_2012}.
In addition, work on human-aware task planning for mobile robots
\cite{cirillo_human-aware_2009} has used \emph{predicted} plans of humans to
guide the system's own planning.
\cite{talamadupula_coordination_2014,chakraborti_planning_2015} showed this
approach more explicitly, representing and reasoning over a subset
of the humans' mental states relevant to the autonomous system's planning
problems.
Recent work has adapted these ideas to proactive decision making
\cite{sengupta_radar_2017,kim_towards_2017} and smart-room environments
\cite{chakraborti_mr._2017}. \cite{pearce_etal_social_planning_aaai2014}
note the importance of what they call ``social planning,'' which
includes an agent achieving a goal via the modification of the mental
states of others.\footnote{Our example here is briefly returned
to later in ``Prior/Related Work and Novelty.''}

While these papers confirm the importance of formalizing and reasoning
about mental states within the planning process, they seem to us to lack the formal and
computational machinery needed to mechanize a full human-level theory
of mind --- let alone such a theory of mind \emph{and} the
requirements of a CAIS that we set out below. Note that in
this work, there is a limit to the theory-of-mind-modeling
``power'' we insist an overseeing AI have.  For example, we don't require
that an AI overseeing an environment populated with humans have
so-called \textit{phenomenal consciousness}, a form of ``what's it's
like to'' consciousness characterized by \cite{bbs.block}, and
claimed by \cite{sb_billion_conscious_robot}, to be impossible for a
mere machine to possess.  In sharp contrast with phenomenal
consciousness, \textit{cognitive consciousness} consists only in the
logico-mathematical \emph{structure} of human-level (and, indeed,
above) cognition, instantiated through time.  While cognitive
consciousness can be characterized axiomatically with help from the
formal languages we introduce below for cognitive calculi
\cite{axiomatizing_consciousness1}, in the present paper we do not
require the AI overseeing a CAIS have even cognitive consciousness,
and we specifically do not require, at this early point in our work
on cognitive-and-immersive systems, cognitive
\emph{self}-consciousness, despite the fact that the latter is
something that has been significantly mechanized and implemented
\cite{roman2015_robot_self-con,sb_on_knowledge_game}.